"""
Sunindu Data Inc.
Scripted by: Alok Kumar
Date: 01-10-2018
Script# : 3

INPUT: Raw Data File generated by Amazon Seller Central Portal for a particular seller that has been fed as webpage input by registered User AND undergone 'ordersFile_preprocessing' script.
OUTPUT: Delivers data for Information Dashboard as a Pandas DataFrame or TextParserself. Generalization and aggregation done to generate new columns as assigned in sequence by the defined functions for Information DashBoard to populate. Same columns with such values shall also be used to generate graph/plots for illustration.
"""

# Importing external dependencies:
import numpy as np
import pandas as pd
from datetime import datetime
from datetime import date
from datetime import time


# Ascertaining quantity of Items purchased per day:
def purchaseQuantity_perDate_month(data):
    """
    DOCSTRING: Creates a 'Purchase Quantity per Day' column based on 'Purchase Date' and 'Purchase Quantity' columns, to assist future aggregations and plotting computations. Another new column 'Purchase Quantity per Month' gets created based on 'Month' and 'Purchase Quantity'. These generated columns holds total items sold for each date and month respectively.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser.

    OUTPUT:
    Pandas DataFrame or TextParser with a new column added for assessing total number of items sold per date and month.
    """
    data["Purchase Quantity per Day"] = data.groupby("Purchase Date")["Purchase Quantity"].transform("sum")
    data["Purchase Quantity per Month"] = data.groupby("Month")["Purchase Quantity"].transform("sum")
    return data


# Ascertaining quantity of Items purchased per day:
def totalEarnings_perDate_month(data):
    """
    DOCSTRING: Creates a 'Total Earnings per Day' column based on 'Purchase Date' and 'Selling Price' (one without 'Currency' format) columns, to assist future aggregations and plotting computations. Another new column 'Total Earnings per Month' gets created based on 'Month' and 'Selling Price'. These generated columns hold total earnings for each date and month respectively.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser.

    OUTPUT:
    Pandas DataFrame or TextParser with 2 new columns added for assessing Total earnings based on date and month.
    """
    data["Total Earnings per Day"] = data.groupby("Purchase Date")["Selling Price"].transform("sum")
    data["Total Earnings per Month"] = data.groupby("Month")["Selling Price"].transform("sum")
    return data



# Fetching SKUs sold per month. Partially failed Operation of adding Fast/Slow moving Items:
def fastSlow_monthlyItems(data):
    """
    DOCSTRING:
    [SUCCESS]:
    [FAILED]: Generating two more columns 'Top 5 Monthly Product' and 'Top 5 Monthly SKU' to indicate products and SKUs that are among top selling items on a monthly basis. Currently a failed operation, hence an alternative required.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser.

    OUTPUT:
    Pandas DataFrame or TextParser with additional column to highlight SKUs sold per month. Two new columns supposed to be added for assessing Top 5 Fast/Slow moving Product and SKU based on 'Monthly SKU Sold' and 'Month'.
    """
    data["Monthly SKU Sold"] = data.groupby(["Month","SKU"])["Purchase Quantity"].transform("sum")
    """
    data["Top 5 Monthly Product"] = np.nan
    # Fetches Top-5 (Fastest) selling Item SKU, separately for each month:
    temp_top_sku = data.sort_values(by="Monthly SKU Sold", ascending=False, kind="mergesort").groupby(["Month"])["SKU"].unique().head(5)
    # Fetches Top-5 (Fastest) selling Product Name, separately for each month:
    temp_bottom_sku = data.sort_values(by="Monthly SKU Sold", ascending=False, kind="mergesort").groupby(["Month"])["SKU"].unique().head(5)
    # Fetches Bottom-5 (Slowest) moving Item SKU, separately for each month:
    temp_top_product = data.sort_values(by="Monthly SKU Sold", kind="mergesort").groupby(["Month"])["SKU"].unique().head(5)
    # Fetches Bottom-5 (Slowest) moving Product Name, separately for each month:
    temp_bottom_product = data.sort_values(by="Monthly SKU Sold", kind="mergesort").groupby(["Month"])["SKU"].unique().head(5)
    # Creating 'Top 5 Monthly Product' column:
    for i,v in data["Product"].iteritems():
        if v in temp_top_product:
            data.loc[i, "Top 5 Monthly Product"] = "Top 5 Product"
        elif v in temp_bottom_product:
            data.loc[i, "Top 5 Monthly Product"] = "Bottom 5 Product"
    # Creating 'Top 5 Monthly SKU' column:
    for i,v in data["SKU"].iteritems():
        if v in temp_top_sku:
            data.loc[i, "Top 5 Monthly SKU"] = "Top 5 SKU"
        elif v in temp_bottom_sku:
            data.loc[i, "Top 5 Monthly SKU"] = "Bottom 5 SKU"
    """
    return data


# Ascertaining Single/Bulk Orders:
def single_bucket_bulk_order(data):
    """
    DOCSTRING: Creates a 'Single/Bulk Order' column based on 'Purchase Quantity' and our LEGB dictionary items of 'order_type'. For Quantity=1, 'Single Order' gets assigned, for Quantity in [2,9], 'Bucket Order' gets assigned as value. For 'Purchase Quantity' of 10 and above, value 'Bulk Order' gets assigned for thaat particular transaction. Datatype remains String.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser.

    OUTPUT:
    Pandas DataFrame or TextParser with 1 additional column for categorizing Order Type based on Purchase Quantity.
    """
    order_type = {1:"Single Order",2:"Bucket Order",3:"Bucket Order",4:"Bucket Order",5:"Bucket Order",6:"Bucket Order",7:"Bucket Order",8:"Bucket Order",9:"Bucket Order",10:"Bulk Order",11:"Bulk Order",12:"Bulk Order",13:"Bulk Order",14:"Bulk Order",15:"Bulk Order",16:"Bulk Order",17:"Bulk Order",18:"Bulk Order",19:"Bulk Order",20:"Bulk Order",21:"Bulk Order",22:"Bulk Order",23:"Bulk Order",24:"Bulk Order",25:"Bulk Order"}
    data["Single/Bucket/Bulk"] = np.nan
    for i,v in data["Purchase Quantity"].iteritems():
        if v in order_type:
            data.loc[i, "Single/Bucket/Bulk"] = order_type[v]
    return data


# Performing data integrity check:
def removing_duplicate_columns(data):
    """
    DOCSTRING: Operation is more of a Data Integrity check at this stage. Purpose is to find and eliminate duplicate COLUMNS only, by running through data columns and related DataTypes.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser that has been pre-processed earlier.

    OUTPUT: Pandas DataFrame or TextParser if there are no duplicate columns, or else throws a Warning message with duplicate column names detected.
    """
    groups = data.columns.to_series().groupby(data.dtypes).groups
    dups = []

    for t,v in groups.items():

        cs = data[v].columns
        vs = data[v]
        lcs = len(cs)

        for i in range(lcs):
            iv = vs.iloc[:,i].tolist()
            for j in range(i+1, lcs):
                jv = vs.iloc[:,j].tolist()
                if iv == jv:
                    dups.append(cs[i])
                    break

    if dups != []:
        return ("[WARNING] Few Columns names in this data are duplicates. Please re-upload data after taking a look at following columns:  " + "".join(dups))
    else:
        return data


# Restructuring Column names for better aesthetics:
def state_col_restructuring(data):
    """
    DOCSTRING: Restructuring upper and lowercasing mix in 'State' and 'City' columns for ease of representation. Yields two columns, namely 'Reformed State' and 'Reformed City'.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser, that has been pre-processed earlier.

    OUTPUT: Pandas DataFrame or TextParser with 2 additional columns, i.e. 'Reformed State' and 'Reformed City', with restructured 'State' and 'City' column values.
    """
    data["Reformed State"] = np.nan
    data["Reformed City"] = np.nan

    # State Column Refinement (Revisit if Indian Government adds new State):
    for i,v in data["State"].iteritems():
        if len(v.split()) == 1:
            new_state = "{}{}".format(v[0].upper(), v[1:].lower())
            data.loc[i, "Reformed State"] = new_state
        elif len(v.split()) == 2:
            new_state = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower())
            data.loc[i, "Reformed State"] = new_state
        elif len(v.split()) == 3:
            new_state = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + v.split()[1].lower() + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower())
            data.loc[i, "Reformed State"] = new_state
        elif len(v.split()) == 4:
            new_state = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + v.split()[1].lower() + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower()) + "{}{}".format(v.split()[3][0].upper(), v.split()[3][1:].lower())
            data.loc[i, "Reformed State"] = new_state
        elif len(v.split()) == 5:
            new_state = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower()) + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower()) + "{}{}".format(v.split()[3][0].upper(), v.split()[3][1:].lower()) + " " + "{}{}".format(v.split()[4][0].upper(), v.split()[4][1:].lower())
            data.loc[i, "Reformed State"] = new_state
        else:
            break

    # State Column Refinement (Revisit if Indian Government assigns new Cities):
    for i,v in data["City"].iteritems():
        if len(v.split()) == 1:
            new_city = "{}{}".format(v[0].upper(), v[1:].lower())
            data.loc[i, "Reformed City"] = new_city
        elif len(v.split()) == 2:
            new_city = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower())
            data.loc[i, "Reformed City"] = new_city
        elif len(v.split()) == 3:
            new_city = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower()) + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower())
            data.loc[i, "Reformed City"] = new_city
        elif len(v.split()) == 4:
            new_city = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower()) + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower()) + "{}{}".format(v.split()[3][0].upper(), v.split()[3][1:].lower())
            data.loc[i, "Reformed City"] = new_city
        elif len(v.split()) == 5:
            new_city = "{}{}".format(v.split()[0][0].upper(), v.split()[0][1:].lower()) + " " + "{}{}".format(v.split()[1][0].upper(), v.split()[1][1:].lower()) + " " + "{}{}".format(v.split()[2][0].upper(), v.split()[2][1:].lower()) + "{}{}".format(v.split()[3][0].upper(), v.split()[3][1:].lower()) + " " + "{}{}".format(v.split()[4][0].upper(), v.split()[4][1:].lower())
            data.loc[i, "Reformed City"] = new_city
        else:
            break

    return data

# Classifying cities as Metro/Premium/Others:
def metro_premium_city(data):
    """
    DOCSTRING: Classifies 'Reformed City' column and accordingly creates another column 'Metro/Premium City' to classify shipping location in either 'Metro' or 'Premium' or 'Other' city. Operation drafted to aid visualization and information boards. Special care has been taken to handle spelling mistakes. [Currently restricted geographically to India].
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser, that has been pre-processed earlier.

    OUTPUT: Pandas DataFrame or TextParser with 1 additional column, i.e. 'Metro/Premium City'.
    """
    metro = ["new delhi", "delhi", "mumbai", "bombay", "kolkata", "kolkatta", "calcutta", "kolkatta", "madras", "chennai", "chenai"]
    premium = ["hyderabad", "hyderbad", "hydrabad", "hyderabaad", "amaravati", "amravati", "amaravatti", "itanagar", "itangr", "dispur", "patna", "raipur", "rapur", "panaji", "panji", "gandhinagar", "gandinagar", "gandhinagr", "gandhingar", "chandigarh", "chandigrh", "shimla","simla","cimla","chimla","srinagar & jammu","srinagar and jammu","srinagar & jamu","srinagr & jammu","srinagar","jammu","shrinagar","shrinagar & jammu","shreenagar & jammu", "shringr & jammu","shringar & jammu","ranchi","bangalore","bengaluru","banglore","bengaloru","bungalore","bungaluru","bungluru","thiruvananthapuram","tiruvantpuram","thiruvnantpuram","bhopal","imphal","impal","shillong","sillong","shilong","aizawi","aizwi","kohima","bhubaneshwara","bhubneswar","bhubneshwar","bhubaneswar","jaipur","japur","gangtok","gngtok","gangtak","agartala","agartla","agratala","agartalla","agrtalla","dehradun","lucknow","luckhnow","lcknow","lckhnow","port blair","portblair","port blar","portblar","silvassa","silvasa","silvsa","daman","dman","damaan","kavaratti","kavratti","kavarati","kavartti","pondicherry","pondichery","pondicherri","pondichhery","pune","pune city","punecity","ahmedabad","ahemedabad","ahmedabaad","indore","indure","cuttack","cuttak"]
    data["Metro/Premium City"] = np.nan
    for i,v in data["Reformed City"].iteritems():
        if v.lower() in metro:
            data.loc[i, "Metro/Premium City"] = "Metro City"
        elif v.lower() in premium:
            data.loc[i, "Metro/Premium City"] = "Premium City"
        else:
            data.loc[i, "Metro/Premium City"] = "Others"
    return data


# List of Customer Phone numbers (might be helpful for Sales&Marketing):
phn_num = []
# Classifying customers as Repeat/New/Unknown customers:
def repeat_new_unknown(data):
    """
    DOCSTRING: Classifies Customers based on buyer phone number, i.e. 'Phone No.' as 'Repeat', 'New' or 'Unknown' customers. Important to note is that this classification ONLY takes into consideration the history of data available to our system. So if customer did make a purchase earlier but transaction doesn't reflect in Seller uploaded data, then customer shall be treated as 'New'.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser, that has been pre-processed earlier.

    OUTPUT: Pandas DataFrame or TextParser with 1 additional column, i.e. 'Repeat/New Buyer'.
    """
    data["Repeat/New Buyer"] = np.nan
    for i,v in data["Phone No."].iteritems():
        if v == "Online":
            data.loc[i, "Repeat/New Buyer"] = "Unknown"
        elif v != "Online":
            if v in phn_num:
                data.loc[i, "Repeat/New Buyer"] = "Repeat"
            else:
                data.loc[i, "Repeat/New Buyer"] = "New"
                phn_num.append(v)

    for i,v in data["Shipping Phone No."].iteritems():
        if v != "Online":
            if data["Repeat/New Buyer"].loc[i] == "Unknown":
                data.loc[i, "Repeat/New Buyer"] = "New"
                phn_num.append(v)

    return data


# Checking if item was promoted/discounted while purchasing:
def discounted_item(data):
    """
    DOCSTRING: Classifies item purchases as 'Promoted' or 'Not Promoted' based on 'Item Discount' column. Also 'COD Collectibles' column gets restructured by eliminating undesired default values, like 'Online'.
    INPUT:
    > data : Only accepts Pandas DataFrame or TextParser, that has been pre-processed earlier.

    OUTPUT: Pandas DataFrame or TextParser with 1 additional column, i.e. 'On Promotion'.
    """
    data["On Promotion"] = np.nan
    data["Phone num"] = np.nan
    data["COD Collectible"] = np.nan   # Later again gets renamed within this func.
    for i,v in data["Item Discount"].iteritems():
        if v != 0:
            data.loc[i, "On Promotion"] = "Promoted"
        else:
            data.loc[i, "On Promotion"] = "Not Promoted"
    # Also taking care of COD Collectible:
    for i,v in data["COD Collectibles"].iteritems():
        if v == "Online":
            data.loc[i, "COD Collectible"] = 0
        else:
            data.loc[i, "COD Collectible"] = v
    # Also taking care of 'Phone No.' column:
    for i,v in data["Phone No."].iteritems():
        if v == "Online":
            data.loc[i, "Phone num"] = "Unavailable"
        else:
            data.loc[i, "Phone num"] = v

    data.drop(["COD Collectibles"], axis=1, inplace=True)
    data.drop(["Phone No."], axis=1, inplace=True)
    data.rename(columns={"COD Collectible": "COD Collectibles"}, inplace=True)
    data.rename(columns={"Phone num": "Phone No."}, inplace=True)
    return data
