"""
Sunindu Data Inc.
Scripted by: Alok Kumar
Date: 04-10-2018

INPUT: 'Payments' raw Data File generated by Amazon Seller Central Portal for a particular seller that has been fed as webpage input by registered User.
OUTPUT: Payments data as a Pandas DataFrame or TextParserself. Columns are renamed and processed for better readability, if they match our internal records.
"""

# Importing required libraries:
import numpy as np
import pandas as pd

# Assigning variable name (avoiding public exposure) to location our sample Orders file:
dummy_file = "./static/datahub/payments.tsv"
ur = "https://drive.google.com/file/d/0B4pWvl2kSrk2Q2lQTDRuV1JNN3ljQkh0TnFoSGxkRmdPcW53/view?usp=sharing"


# [DO NOT TOUCH] Renaming columns:
expected_columns = {'Order date':'Purchase Dat', 'Return request date':'Return Request Dat', 'Return request status':'Return Request Status', 'Label type':'Label Type', 'Label cost':'Label Cost', 'Currency code':'Currency', 'Return carrier':'Return Carrier', 'Label to be paid by':'Label Payer', 'A to z claim':'A-Z Claim', 'Is prime':'Is Prime', 'Merchant SKU':'SKU', 'Item name':'Product', 'Return quantity':'Return Quantity', 'Return reason':'Return Reason', 'In policy':'In Policy', 'Return type':'Return Type', 'Invoice number':'Invoice No.', 'Return delivery date':'Return Delivery Dat'}

# Dictionary of months of an Year. Used within this script for 'treat_missing_date':
monthy = {'Jan':'01','Feb':'02','Mar':'03','Apr':'04','May':'05','Jun':'06','Jul':'07','Aug':'08','Sep':'09','Oct':'10','Nov':'11','Dec':'12'}


## Parses webpage input file to generate Pandas DataFrame:
def file_parser(localpath = None, url = None, sep = " ", delimiter = "\t"):
    """
    DOCSTRING:
    INPUT:
    > 'localpath' : String (str). Ideally expects a local object with a read() method (such as a file handle or StringIO).
    By default, 'localpath=dummy_file' parameter can be passed to auto-detect and parse one of our dummy 'Payments' file in Amazon format. Acceptable input file extensions include .CSV, .TSV and .TXT. Needs to be passed in within quotes, either single or double quotes. Default 'dummy_file' doesn't require additional quotes.
    > 'url' : [OPTIONAL] String (str). If supplied with value, 'localpath' needs to be left at default 'None' or else shall output an error message. Expected file type contained within URL should be either in .CSV, .TSV and .TXT format. Needs to be passed in within quotes, either single or double quotes. Default 'url=ur' can be passed w/o additional quotes for fetching dummy data.
    > 'sep' : [OPTIONAL] String (str). Optional, and isn't expected to be modified unless critical. Powered by Pythonâ€™s builtin parsing sniffer tool.
    In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex separators are prone to ignoring quoted data. [Regex example: '\r\t'].
    > 'delimiter' : [OPTIONAL] String (str). Parameter isn't expected to be modified (Like setting to 'None') unless critical. Alternative argument name for previous argument 'sep', so a careful choice needs to be made.

    OUTPUT:
    Shall result into a Pandas DataFrame or TextParser for further data processing.
    """
    # Checking existence of 'filepath' or 'url' parameter before parsing:
    if localpath == None and url == None:
        return "Please input EITHER local file path to 'localpath' parameter OR any valid readable URL to 'url' parameter"
    elif localpath != None and url == None:
        if localpath.lower().endswith((".txt", ".csv", ".tsv")):
            data = pd.read_csv(localpath, sep = sep, delimiter=delimiter, parse_dates=[0], infer_datetime_format=True)
            return data
        else:
            return "This file format is not supported. Kindly refer to our functional flow documentation for further assistance!"
    elif localpath == None and url != None:
        data = pd.read_csv(url, sep = sep, delimiter=delimiter, parse_dates=[0], infer_datetime_format=True)
        return data
    else:
        return "Please pass valid input for processing."


## Validating current DataFrame Column Names against Column names in our records and accordingly renaming specific columns:
def columns_renamer(data):
    """
    DOCSTRING:
    INPUT:
    > 'data' : Only accepts Pandas DataFrame or TextParser. Operation dependancy on 'expected_columns' dictionary created globally in this script.

    OUTPUT:
    Pandas DataFrame with modified column names as per our internal records.
    """
    print("[INFO] Columns being modified are... ")
    for col in data.columns:
        # Dictionary copy is used to avoid threaded coding issues during iteration:
        if col in list(expected_columns.copy().keys()):
            data.rename({col:expected_columns[col]}, axis=1, inplace=True)
            # Below line isn't required in prod env:
            print(col, sep=" ,", end= "", flush=True)
    return data


## Treating missing values if any. Columns might get re-adjusted later so this is preliminary action step:
def treat_missing_date(data):
    """
    DOCSTRING: Empty values in certain columns ['Amazon RMA ID','Currency','Return Carrier','Return Delivery Date'] get replaced with 'Unavailable'.
    Further 'Purchase Dat', 'Return Request Dat' and 'Return Delivery Dat' columns get modified to DD-MM-YYYY format, to sync with Amazon 'Orders' file in our internal records. Reformed column names are 'Purchase Date', 'Return Request Date' and 'Return Delivery Date'. These columns are for better representation in plotting.
    [IMPORTANT] Finally, 2 columns are additionally re-created ONLY for internal calculations in 'datetime64' datatype, as 'Order Date' and 'Return Date'.
    INPUT:
    > 'data' : Only accepts Pandas DataFrame or TextParser.

    OUTPUT:
    Pandas DataFrame with column-wise missing value replacements.
    """
    # Replacing empty calues in following columns:
    data["Amazon RMA ID"].replace(" ", "Unavailable", inplace=True)
    data["Currency"].replace(" ", "Unavailable", inplace=True)
    data["Return Carrier"].replace(" ", "Unavailable", inplace=True)
    data["Return Delivery Dat"].replace(" ", "Unavailable", inplace=True)

    # Modifying date format in 2 columns for internal calculations as specified in Docstring:
    data["order_date"] = pd.to_datetime(data["Purchase Dat"], format="%d-%b-%Y", dayfirst=True)
    data["return_date"] = pd.to_datetime(data["Return Request Dat"], format="%d-%b-%Y", dayfirst=True)

    # [INEFFICIENT] Manual implementation:
    data["Purchase Date"] = np.nan
    for i,v in data["Purchase Dat"].iteritems():
        desired = v.split("-")
        data.loc[i, "Purchase Date"] = (desired[0] + '-' + monthy[desired[1]] + '-' + desired[2])

    data["Return Request Date"] = np.nan
    for i,v in data["Return Request Dat"].iteritems():
        desired = v.split("-")
        data.loc[i, "Return Request Date"] = (desired[0] + '-' + monthy[desired[1]] + '-' + desired[2])

    data["Return Delivery Date"] = np.nan
    for i,v in data["Return Delivery Dat"].iteritems():
        if v != "Unavailable":
            desired = v.split("-")
            data.loc[i, "Return Delivery Date"] = (desired[0] + '-' + monthy[desired[1]] + '-' + desired[2])
    # Dropping all three add-on tables:
    data.drop(["Purchase Dat","Return Request Dat","Return Delivery Dat"], axis=1, inplace=True)
    return data
